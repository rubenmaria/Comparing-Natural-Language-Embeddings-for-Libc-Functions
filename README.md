# Comparing Natural Language Embeddings for Libc Functions as Rich Labels

In recent years, there have been great advances in encoding natural languages in real-valued
vectors. This progress can be used to generate embeddings out of natural language information
in C source code. These embeddings can then be used to train a neural network with supervised
learning. Given a function in binary code, this neural network predicts a vector that reflects the
content of the function. These embeddings are generated by the
[SentenceTransformer](https://sbert.net) using
function names, function comments and [Code Llama](https://github.com/meta-llama/codellama) 
explanations. In addition, we compare
embeddings generated by the [Code2Vec](https://github.com/tech-srl/code2vec) 
model. To classify the quality of Code Llama vectors,
Code2Vec vectors, function comment vectors and function name vectors, they were compared
qualitatively and quantitatively. The best method was identified through an expert survey. It was
found that Code Llama produced the best embeddings. The Code2Vec vectors, function comment
vectors and function name vectors were compared quantitatively with the Code Llama vectors using
a formula. Finally, the Code Llama, Code2Vec, function comment and function name vectors were
compared qualitatively using t-SNE. After qualitative and quantitative evaluation, the embeddings
can be ranked in descending order of quality as follows: Code llama vectors, function name vectors,
function comment vectors and Code2Vec vectors.
